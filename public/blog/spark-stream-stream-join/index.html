<!DOCTYPE html>
<html lang="en-us">

  <head>
    <meta charset="utf-8">
<meta name="robots" content="all,follow">
<meta name="googlebot" content="index,follow,snippet,archive">
<meta name="viewport" content="width=device-width, initial-scale=1">

<title>How to write your first Spark application with Stream-Stream Joins with working code.</title>
<meta name="author" content="Canadian Data Guy" />




<meta name="keywords" content="devcows, hugo, go, checkpoint, streaming, spark streaming">


<meta name="description" content="Have you been waiting to try Streaming Joins but have been unable to take the plunge? In a single blog, we will teach you whatever needs to be understood, along with an end-to-end pipeline, which you can copy-paste and make your own.">

<meta name="generator" content="Hugo 0.120.4">


<link href='//fonts.googleapis.com/css?family=Roboto:400,100,100italic,300,300italic,500,700,800' rel='stylesheet' type='text/css'>


<link rel="stylesheet" href="//use.fontawesome.com/releases/v5.11.2/css/all.css">
<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/css/bootstrap.min.css" integrity="sha384-BVYiiSIFeK1dGmJRAkycuHAHRg32OmUcww7on3RYdg4Va+PmSTsz/K68vbdEjh4u" crossorigin="anonymous">


<link href="/css/animate.css" rel="stylesheet">



  <link href="/css/style.red.css" rel="stylesheet" id="theme-stylesheet">



<link href="/css/custom.css?1701665068" rel="stylesheet">



  <!--[if lt IE 9]>
      <script src="https://oss.maxcdn.com/html5shiv/3.7.2/html5shiv.min.js"></script>
      <script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
  <![endif]-->



<link rel="shortcut icon" href="/img/favicon.ico" type="image/x-icon" />
<link rel="apple-touch-icon" href="/img/apple-touch-icon.png" />


<link href="/css/owl.carousel.css" rel="stylesheet">
<link href="/css/owl.theme.css" rel="stylesheet">


<link rel="alternate" href="https://canadiandataguy.com/index.xml" type="application/rss+xml" title="Canadian Data Guy">







<meta property="og:updated_time" content="2023-03-23T06:32:42Z">

  
  
  <meta property="article:section" content="streaming">
  <meta property="article:tag" content="checkpoint">
  <meta property="article:tag" content="streaming">
  <meta property="article:tag" content="spark streaming">
  
  
  <meta property="article:published_time" content="2023-03-23T06:32:42Z">
  <meta property="article:modified_time" content="2023-03-23T06:32:42Z">



<meta name="twitter:card" content="summary">

<meta name="twitter:title" content="How to write your first Spark application with Stream-Stream Joins …">

<meta name="twitter:description" content="Have you been waiting to try Streaming Joins but have been unable to take the plunge? In a single blog, we will teach you whatever needs to be understood, along with an end-to-end pipeline, which you …">


    
  </head>

  <body>

    <div id="all">

        


        <header class="navbar-affixed-top" data-spy="affix" data-offset-top="62">
    <div class="navbar navbar-default yamm mouseover" role="navigation" id="navbar">
        <div class="container">
            <div class="navbar-header">
                <a class="navbar-brand home" href="/">
                    
                      <img src="/img/cdg5.png" alt="How to write your first Spark application with Stream-Stream Joins with working code. logo" class="hidden-xs hidden-sm" />
                      <img src="/img/cdg5.png" alt="How to write your first Spark application with Stream-Stream Joins with working code. logo" class="visible-xs visible-sm" />
                    
                    <span class="sr-only">How to write your first Spark application with Stream-Stream Joins with working code. - go to homepage</span>
                </a>
                <div class="navbar-buttons">
                    <button type="button" class="navbar-toggle btn-template-main" data-toggle="collapse" data-target="#navigation">
                      <span class="sr-only">Toggle Navigation</span>
                        <i class="fas fa-align-justify"></i>
                    </button>
                </div>
            </div>
            

            <div class="navbar-collapse collapse" id="navigation">
                <ul class="nav navbar-nav navbar-right">
                  

                  
                  
                  

                  

                  

                  

                  
                  <li class="dropdown ">
                    <a href="/">Home</a>
                  </li>
                  
                  
                  
                  

                  

                  
                    
                  

                  

                  
                  <li class="dropdown active">
                    <a href="/blog/">Blog</a>
                  </li>
                  
                  
                  
                  

                  

                  

                  

                  
                    
                    
                    <li class="dropdown ">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Portfolio <span class="caret"></span></a>
                        
                        <ul class="dropdown-menu">
                            
                            <li><a href="/databricks-customer-success-stories/">Databricks Customer Success Stories</a></li>
                            
                        </ul>
                        
                    </li>
                  
                  
                  
                  

                  

                  

                  

                  
                    
                    
                    <li class="dropdown ">
                        <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-haspopup="true" aria-expanded="false">Interview Coaching <span class="caret"></span></a>
                        
                        <ul class="dropdown-menu">
                            
                            <li><a href="/blog/youtube/">Youtube</a></li>
                            
                            <li><a href="/blog/howtoprepareyourselftobebetteratdatainterviews">how to prepare for data interviews</a></li>
                            
                            <li><a href="/blog/1-on-1-coaching/">1 on 1 coaching</a></li>
                            
                        </ul>
                        
                    </li>
                  
                  
                  
                  

                  

                  

                  

                  
                  <li class="dropdown ">
                    <a href="/about-me/">About Me</a>
                  </li>
                  
                  
                  
                  

                  

                  

                  

                  
                  <li class="dropdown ">
                    <a href="/contact/">Contact</a>
                  </li>
                  
                  
                </ul>
            </div>
            

            <div class="collapse clearfix" id="search">
                <form class="navbar-form" role="search">
                    <div class="input-group">
                        <input type="text" class="form-control" placeholder="Search">
                        <span class="input-group-btn">
                    <button type="submit" class="btn btn-template-main"><i class="fas fa-search"></i></button>
                </span>
                    </div>
                </form>
            </div>
            
        </div>
    </div>
</header>




        <div id="heading-breadcrumbs">
    <div class="container">
        <div class="row">
            <div class="col-md-12">
                <h1>How to write your first Spark application with Stream-Stream Joins with working code.</h1>
            </div>
        </div>
    </div>
</div>


        <div id="content">
            <div class="container">

                <div class="row">

                    

                    <div class="col-md-9" id="blog-post">

                        
                          <p class="text-muted text-uppercase mb-small text-right">
                            
                            
                            
                              
                              
                                  
                              
                                  
                              
                                  
                              
                                  
                              
                                  
                              
                                  
                              
                                  
                              
                                  
                              
                                  
                              
                                  
                              
                                  
                              
                                  
                              
                              March 23, 2023
                            
                          </p>
                        

                        <div id="post-content">
                          <h2 id="how-to-write-your-first-spark-application-with-stream-stream-joins-with-working-code">How to write your first Spark application with Stream-Stream Joins with working code.</h2>
<p>Have you been waiting to try Streaming but cannot take the plunge?</p>
<p>In a single blog, we will teach you whatever needs to be understood about Streaming Joins. We will give you a working code which you can use for your next Streaming Pipeline.</p>
<p>The steps involved:</p>
<ol>
<li>Create a fake dataset at scale</li>
<li>Set a baseline using traditional SQL</li>
<li>Define Temporary Streaming Views</li>
<li>Inner Joins with optional Watermarking</li>
<li>Left Joins with Watermarking</li>
<li>The cold start edge case: withEventTimeOrder</li>
<li>Cleanup</li>
</ol>
<p><img src="https://cdn-images-1.medium.com/max/2560/1*eds6wrWUDMfdL9I48-heFQ.jpeg" alt=""></p>
<h2 id="what-is-stream-stream-join">What is Stream-Stream Join?</h2>
<p>Stream-stream join is a widely used operation in stream processing where two or more data streams are joined based on some common attributes or keys. It is essential in several use cases, such as real-time analytics, fraud detection, and IoT data processing.</p>
<h3 id="concept-of-stream-stream-join">Concept of Stream-Stream Join</h3>
<p>Stream-stream join combines two or more streams based on a common attribute or key. The join operation is performed on an ongoing basis, with each new data item from the stream triggering a join operation. In stream-stream join, each data item in the stream is treated as an event, and it is matched with the corresponding event from the other stream based on matching criteria. This matching criterion could be a common attribute or key in both streams.</p>
<p>When it comes to joining data streams, there are a few key challenges that must be addressed to ensure successful results. One of the biggest hurdles is the fact that, at any given moment, neither stream has a complete view of the dataset. This can make it difficult to find matches between inputs and generate accurate join results.</p>
<p>To overcome this challenge, it’s important to buffer past input as a streaming state for both input streams. This allows for every future input to be matched with past input, which can help to generate more accurate join results. Additionally, this buffering process can help to automatically handle late or out-of-order data, which can be common in streaming environments.</p>
<p>To further optimize the join process, it’s also important to use watermarks to limit the state. This can help to ensure that only the most relevant data is being used to generate join results, which can help to improve accuracy and reduce processing times.</p>
<h3 id="types-of-stream-stream-join">Types of Stream-Stream Join</h3>
<p>Depending on the nature of the join and the matching criteria, there are several types of stream-stream join operations. Some of the popular types of stream-stream join are:</p>
<p><strong>Inner Join</strong>
In inner join, only those events are returned where there is a match in both the input streams. This type of join is useful when combining the data from two streams with a common key or attribute.</p>
<p><strong>Outer Join</strong>
In outer join, all events from both the input streams are included in the joined stream, whether or not there is a match between them. This type of join is useful when we need to combine data from two streams, and there may be missing or incomplete data in either stream.</p>
<p><strong>Left Join</strong>
In left join, all events from the left input stream are included in the joined stream, and only the matching events from the right input stream are included. This type of join is useful when we need to combine data from two streams and keep all the data from the left stream, even if there is no matching data in the right stream.</p>
<h2 id="1-the-setup-create-a-fake-dataset-at-scale">1. The Setup: Create a fake dataset at scale</h2>
<p>Most people do not have 2 streams just hanging around for one to experiment with Stream Steam Joins. Thus I used Faker to mock 2 different streams which we will use for this example.</p>
<p>The name of the library being used is Faker and faker_vehicle to create Datasets.</p>
<pre><code>!pip install faker_vehicle
!pip install faker
</code></pre>
<p>Imports</p>
<pre><code>from faker import Faker
from faker_vehicle import VehicleProvider
from pyspark.sql import functions as F
import uuid
from utils import logger
</code></pre>
<p>Parameters</p>
<pre><code># define schema name and where should the table be stored
schema_name = “test_streaming_joins”
schema_storage_location = “/tmp/CHOOSE_A_PERMANENT_LOCATION/”
</code></pre>
<p><strong>Create the Target Schema/Database</strong>
Create a Schema and set location. This way, all tables would inherit the base location.</p>
<pre><code>create_schema_sql = f”””
 CREATE SCHEMA IF NOT EXISTS {schema_name}
 COMMENT ‘This is {schema_name} schema’
 LOCATION ‘{schema_storage_location}’
 WITH DBPROPERTIES ( Owner=’Jitesh’);
 “””
print(f”create_schema_sql: {create_schema_sql}”)
spark.sql(create_schema_sql)
</code></pre>
<p>Use Faker to define functions to help generate fake column values</p>
<pre><code>fake = Faker()
fake.add_provider(VehicleProvider)

event_id = F.udf(lambda: str(uuid.uuid4()))
vehicle_year_make_model = F.udf(fake.vehicle_year_make_model)
vehicle_year_make_model_cat = F.udf(fake.vehicle_year_make_model_cat)
vehicle_make_model = F.udf(fake.vehicle_make_model)
vehicle_make = F.udf(fake.vehicle_make)
vehicle_model = F.udf(fake.vehicle_model)
vehicle_year = F.udf(fake.vehicle_year)
vehicle_category = F.udf(fake.vehicle_category)
vehicle_object = F.udf(fake.vehicle_object)

latitude = F.udf(fake.latitude)
longitude = F.udf(fake.longitude)
location_on_land = F.udf(fake.location_on_land)
local_latlng = F.udf(fake.local_latlng)
zipcode = F.udf(fake.zipcode)
</code></pre>
<p>Generate Streaming source data at your desired rate</p>
<pre><code>def generated_vehicle_and_geo_df (rowsPerSecond:int , numPartitions :int ):
    return (
        spark.readStream.format(&quot;rate&quot;)
        .option(&quot;numPartitions&quot;, numPartitions)
        .option(&quot;rowsPerSecond&quot;, rowsPerSecond)
        .load()
        .withColumn(&quot;event_id&quot;, event_id())
        .withColumn(&quot;vehicle_year_make_model&quot;, vehicle_year_make_model())
        .withColumn(&quot;vehicle_year_make_model_cat&quot;, vehicle_year_make_model_cat())
        .withColumn(&quot;vehicle_make_model&quot;, vehicle_make_model())
        .withColumn(&quot;vehicle_make&quot;, vehicle_make())
        .withColumn(&quot;vehicle_year&quot;, vehicle_year())
        .withColumn(&quot;vehicle_category&quot;, vehicle_category())
        .withColumn(&quot;vehicle_object&quot;, vehicle_object())
        .withColumn(&quot;latitude&quot;, latitude())
        .withColumn(&quot;longitude&quot;, longitude())
        .withColumn(&quot;location_on_land&quot;, location_on_land())
        .withColumn(&quot;local_latlng&quot;, local_latlng())
        .withColumn(&quot;zipcode&quot;, zipcode())
        )

# You can uncomment the below display command to check if the code in this cell works
#display(generated_vehicle_and_geo_df)

# You can uncomment the below display command to check if the code in this cell works
#display(generated_vehicle_and_geo_df)
</code></pre>
<p>Now let&rsquo;s generate the base source table and let’s call it Vehicle_Geo</p>
<pre><code>table_name_vehicle_geo= &quot;vehicle_geo&quot;
def stream_write_to_vehicle_geo_table(rowsPerSecond: int = 1000, numPartitions: int = 10):
    
    (
        generated_vehicle_and_geo_df(rowsPerSecond, numPartitions)
            .writeStream
            .queryName(f&quot;write_to_delta_table: {table_name_vehicle_geo}&quot;)
            .option(&quot;checkpointLocation&quot;, f&quot;{schema_storage_location}/{table_name_vehicle_geo}/_checkpoint&quot;)
            .format(&quot;delta&quot;)
            .toTable(f&quot;{schema_name}.{table_name_vehicle_geo}&quot;)
    )
stream_write_to_vehicle_geo_table(rowsPerSecond = 1000, numPartitions = 10)
</code></pre>
<p>Let the above code run for a few iterations, and you can play with rowsPerSecond and numPartitions to control how much data you would like to generate. Once you have generated enough data, kill the above stream and get a base line for row count.</p>
<pre><code>spark.read.table(f&quot;{schema_name}.{table_name_vehicle_geo}&quot;).count()

display(
    spark.sql(f&quot;&quot;&quot;
    SELECT * 
    FROM {schema_name}.{table_name_vehicle_geo}
&quot;&quot;&quot;)
)
</code></pre>
<p>Let’s also get a min &amp; max of the timestamp column as we would be leveraging it for watermarking.</p>
<pre><code>display(
    spark.sql(f&quot;&quot;&quot;
    SELECT 
         min(timestamp)
        ,max(timestamp)
        ,current_timestamp()
    FROM {schema_name}.{table_name_vehicle_geo}
&quot;&quot;&quot;)
)
</code></pre>
<h3 id="next-we-will-break-this-delta-table-into-2-different-tables">Next, we will break this Delta table into 2 different tables</h3>
<p>Because for Stream-Stream Joins we need 2 different streams. We will use Delta To Delta Streaming here to create these tables.</p>
<ol>
<li><strong>a ) Table: Vehicle</strong></li>
</ol>
<pre tabindex="0"><code>table_name_vehicle = &#34;vehicle&#34;
vehicle_df = (
    spark.readStream.format(&#34;delta&#34;)
    .option(&#34;maxFilesPerTrigger&#34;, &#34;100&#34;)
    .table(f&#34;{schema_name}.vehicle_geo&#34;)
    .selectExpr(
        &#34;event_id&#34;,
        &#34;timestamp as vehicle_timestamp&#34;,
        &#34;vehicle_year_make_model&#34;,
        &#34;vehicle_year_make_model_cat&#34;,
        &#34;vehicle_make_model&#34;,
        &#34;vehicle_make&#34;,
        &#34;vehicle_year&#34;,
        &#34;vehicle_category&#34;,
        &#34;vehicle_object&#34;,
    )
)


def stream_write_to_vehicle_table():

    (
        vehicle_df.writeStream
        # .trigger(availableNow=True)
        .queryName(f&#34;write_to_delta_table: {table_name_vehicle}&#34;)
        .option(
            &#34;checkpointLocation&#34;,
            f&#34;{schema_storage_location}/{table_name_vehicle}/_checkpoint&#34;,
        )
        .format(&#34;delta&#34;)
        .toTable(f&#34;{schema_name}.{table_name_vehicle}&#34;)
    )


stream_write_to_vehicle_table()
</code></pre><ol>
<li><strong>b) Table: Geo</strong></li>
</ol>
<p>We have added a filter when we write to this table. This would be useful when we emulate the left join scenario. Filter: where(&ldquo;value like &lsquo;1%&rsquo; &ldquo;)</p>
<pre><code>geo_df = (
    spark.readStream.format(&quot;delta&quot;).option(&quot;maxFilesPerTrigger&quot;,&quot;100&quot;).table(f&quot;{schema_name}.vehicle_geo&quot;)
        .selectExpr(
            &quot;event_id&quot;
            ,&quot;value&quot;
            ,&quot;timestamp as geo_timestamp&quot;
            ,&quot;latitude&quot;
            ,&quot;longitude&quot;
            ,&quot;location_on_land&quot;
            ,&quot;local_latlng&quot;
            ,&quot;cast( zipcode as integer) as zipcode&quot;
        ).where(&quot;value like '1%' &quot;) 
    )
#geo_df.printSchema()
#display(geo_df)

table_name_geo = &quot;geo&quot;
def stream_write_to_geo_table():
    
    (   geo_df
        .writeStream
        #.trigger(availableNow=True)
        .queryName(f&quot;write_to_delta_table: {table_name_geo}&quot;)
        .option(&quot;checkpointLocation&quot;, f&quot;{schema_storage_location}/{table_name_geo}/_checkpoint&quot;)
        .format(&quot;delta&quot;)
        .toTable(f&quot;{schema_name}.{table_name_geo}&quot;)
    )
    
stream_write_to_geo_table()    
</code></pre>
<h2 id="2-set-a-baseline-using-traditional-sql">2. Set a baseline using traditional SQL</h2>
<p>Before we do the actual streaming joins. Let’s do a regular join and figure out the expected row count.</p>
<p><strong>Get row count from Inner Join</strong></p>
<pre><code>sql_query_batch_inner_join = f'''
        SELECT count(vehicle.event_id) as row_count_for_inner_join
        FROM {schema_name}.{table_name_vehicle} vehicle
        JOIN {schema_name}.{table_name_geo} geo
        ON vehicle.event_id = geo.event_id
    AND vehicle_timestamp &gt;= geo_timestamp  - INTERVAL 5 MINUTES        
        '''
print(f''' Run SQL Query: 
          {sql_query_batch_inner_join}       
       ''')
display( spark.sql(sql_query_batch_inner_join) )
</code></pre>
<p><strong>Get row count from Inner Join</strong></p>
<pre><code>sql_query_batch_left_join = f'''
        SELECT count(vehicle.event_id) as row_count_for_left_join
        FROM {schema_name}.{table_name_vehicle} vehicle
        LEFT JOIN {schema_name}.{table_name_geo} geo
        ON vehicle.event_id = geo.event_id
            -- Assume there is a business logic that timestamp cannot be more than 15 minutes off
    AND vehicle_timestamp &gt;= geo_timestamp  - INTERVAL 5 MINUTES
        '''
print(f''' Run SQL Query: 
          {sql_query_batch_left_join}       
       ''')
display( spark.sql(sql_query_batch_left_join) )
</code></pre>
<h2 id="summary-so-far">Summary so far:</h2>
<ol>
<li>
<p>We created a Source Delta Table: vehicle_geo</p>
</li>
<li>
<p>We took the previous table and divided its column into two tables: Vehicle and Geo</p>
</li>
<li>
<p>Vehicle row count matches with vehicle_geo, and it has a subset of those columns</p>
</li>
<li>
<p>The Geo row count is lesser than Vehicle because we added a filter when we wrote to the Geo table</p>
</li>
<li>
<p>We ran 2 SQL to identify what the row count should be after we do stream-stream join</p>
</li>
</ol>
<h2 id="3-define-temporary-streaming-views">3. Define Temporary Streaming Views</h2>
<p>Some people prefer to write the logic in SQL. Thus, we are creating streaming views which could be manipulated with SQL. The below code block will help create a view and set a watermark on the stream.</p>
<pre><code>def stream_from_delta_and_create_view (schema_name: str, table_name:str, column_to_watermark_on:str, how_late_can_the_data_be: str = &quot;2 minutes&quot; , maxFilesPerTrigger: int = 100):
    view_name = f&quot;_streaming_vw_{schema_name}_{table_name}&quot;
    print(f&quot;Table {schema_name}.{table_name} is now streaming under a temporoary view called {view_name}&quot;)
    (
        spark.readStream.format(&quot;delta&quot;)
        .option(&quot;maxFilesPerTrigger&quot;, f&quot;{maxFilesPerTrigger}&quot;)
        .option(&quot;withEventTimeOrder&quot;, &quot;true&quot;)
        .table(f&quot;{schema_name}.{table_name}&quot;)
        .withWatermark(f&quot;{column_to_watermark_on}&quot;,how_late_can_the_data_be)
        .createOrReplaceTempView(view_name)
    )
</code></pre>
<p><strong>3. a Create Vehicle Stream</strong></p>
<p>Let’s create a Vehicle Stream and set its watermark as 1mins</p>
<pre><code>stream_from_delta_and_create_view(schema_name =schema_name, table_name = 'vehicle', column_to_watermark_on =&quot;vehicle_timestamp&quot;, how_late_can_the_data_be = &quot;1 minutes&quot; )
</code></pre>
<p>Let’s visualize the stream.</p>
<pre><code>display(
    spark.sql(f'''
        SELECT *
        FROM _streaming_vw_test_streaming_joins_vehicle
    ''')
)
</code></pre>
<p>You can also do an aggregation on the stream. It’s out of the scope of this blog, but I wanted to show you how you can do it</p>
<pre><code>display(
    spark.sql(f'''
        SELECT 
            vehicle_make
            ,count(1) as row_count
        FROM _streaming_vw_test_streaming_joins_vehicle
        GROUP BY vehicle_make
        ORDER BY vehicle_make
    ''')
)
</code></pre>
<p><strong>3. b Create Geo Stream</strong></p>
<p>Let’s create a Geo Stream and set its watermark as 2 mins</p>
<pre><code>stream_from_delta_and_create_view(schema_name =schema_name, table_name = 'geo', column_to_watermark_on =&quot;geo_timestamp&quot;, how_late_can_the_data_be = &quot;2 minutes&quot; )
</code></pre>
<p>Have a look at what the data looks like</p>
<pre><code>display(
    spark.sql(f'''
        SELECT *
        FROM _streaming_vw_test_streaming_joins_geo
    ''')
)
</code></pre>
<h2 id="4-inner-joins-with-optional-watermarking">4. Inner Joins with optional Watermarking</h2>
<p>While inner joins on any kind of columns and with any kind of conditions are possible in streaming environments, it’s important to be aware of the potential for unbounded state growth. As new input arrives, it can potentially match with any input from the past, leading to a rapidly increasing streaming state size.</p>
<p>To avoid this issue, it’s essential to define additional join conditions that prevent indefinitely old inputs from matching with future inputs. By doing so, it’s possible to clear old inputs from the state, which can help to prevent unbounded state growth and ensure more efficient processing.</p>
<p>There are a variety of techniques that can be used to define these additional join conditions. For example, you might limit the scope of the join by only matching on a subset of columns, or you might set a time-based constraint that prevents old inputs from being considered after a certain period of time has elapsed.</p>
<p>Ultimately, the key to managing streaming state size and ensuring efficient join processing is to consider the unique requirements of your specific use case carefully and to leverage the right techniques and tools to optimize your join conditions accordingly. <strong>Although watermarking could be optional, I would highly recommend you set a watermark on both streams.</strong></p>
<pre><code>sql_for_stream_stream_inner_join = f&quot;&quot;&quot;
    SELECT 
        vehicle.*
        ,geo.latitude
        ,geo.longitude
        ,geo.zipcode
    FROM _streaming_vw_test_streaming_joins_vehicle vehicle
    JOIN _streaming_vw_test_streaming_joins_geo geo
    ON vehicle.event_id = geo.event_id
    -- Assume there is a business logic that timestamp cannot be more than X minutes off
    AND vehicle_timestamp &gt;= geo_timestamp - INTERVAL 5 minutes
&quot;&quot;&quot;
#display(spark.sql(sql_for_stream_stream_inner_join))

table_name_stream_stream_innner_join ='stream_stream_innner_join'

(   spark.sql(sql_for_inner_join)
    .writeStream
    #.trigger(availableNow=True)
        .queryName(f&quot;write_to_delta_table: {table_name_stream_stream_innner_join}&quot;)
        .option(&quot;checkpointLocation&quot;, f&quot;{schema_storage_location}/{table_name_stream_stream_innner_join}/_checkpoint&quot;)
        .format(&quot;delta&quot;)
        .toTable(f&quot;{schema_name}.{table_name_stream_stream_innner_join}&quot;)
)
</code></pre>
<p>If the stream has finished then in the next step. You should find that the row count should match up with the regular batch SQL Job</p>
<pre><code>spark.read.table(f&quot;{schema_name}.{table_name_stream_stream_innner_join}&quot;).count()
</code></pre>
<h3 id="how-was-the-watermark-computed-in-this-scenario">How was the watermark computed in this scenario?</h3>
<p>When we defined streaming views for Vehicle and Geo, we set them as 1 min and 2 min, respectively.</p>
<p>If you look at the join condition we mentioned :</p>
<pre><code>AND vehicle_timestamp &gt;= geo_timestamp - INTERVAL 5 minutes
</code></pre>
<p>5 min + 2 min = 7 min.</p>
<p>Spark Streaming would automatically calculate this 7 min number and the state would be cleared after that.</p>
<h2 id="5-left-joins-with-watermarking">5. Left Joins with Watermarking</h2>
<p>While the watermark + event-time constraints is optional for inner joins, for outer joins they must be specified. This is because for generating the NULL results in outer join, the engine must know when an input row is not going to match with anything in future. Hence, the watermark + event-time constraints must be specified for generating correct results.</p>
<h3 id="5a-how-left-joins-works-differently-than-an-inner-join">5.a How Left Joins works differently than an Inner Join</h3>
<p>One important factor is that the outer NULL results will be generated with a delay that depends on the specified watermark delay and the time range condition. This delay is necessary to ensure that there were no matches, and that there will be no matches in the future.</p>
<p>In the current implementation of the micro-batch engine, watermarks are advanced at the end of each micro-batch, and the next micro-batch uses the updated watermark to clean up the state and output outer results. However, this means that the generation of outer results may be delayed if there is no new data being received in the stream. If either of the two input streams being joined does not receive data for a while, the outer output (in both left and right cases) may be delayed.</p>
<pre><code>sql_for_stream_stream_left_join = f&quot;&quot;&quot;
    SELECT 
        vehicle.*
        ,geo.latitude
        ,geo.longitude
        ,geo.zipcode
    FROM _streaming_vw_test_streaming_joins_vehicle vehicle
    LEFT JOIN _streaming_vw_test_streaming_joins_geo geo
    ON vehicle.event_id = geo.event_id
    AND vehicle_timestamp &gt;= geo_timestamp  - INTERVAL 5 MINUTES
&quot;&quot;&quot;
#display(spark.sql(sql_for_stream_stream_left_join))

table_name_stream_stream_left_join ='stream_stream_left_join'

(   spark.sql(sql_for_stream_stream_left_join)
    .writeStream
    #.trigger(availableNow=True)
        .queryName(f&quot;write_to_delta_table: {table_name_stream_stream_left_join}&quot;)
        .option(&quot;checkpointLocation&quot;, f&quot;{schema_storage_location}/{table_name_stream_stream_left_join}/_checkpoint&quot;)
        .format(&quot;delta&quot;)
        .toTable(f&quot;{schema_name}.{table_name_stream_stream_left_join}&quot;)
)
</code></pre>
<p>If the stream has finished, then in the next step. You should find that the row count should match up with the regular batch SQL Job.</p>
<pre><code>spark.read.table(f&quot;{schema_name}.{table_name_stream_stream_left_join}&quot;).count()
</code></pre>
<blockquote>
<p>**You will find that some records that could not match are not being released, which is expected. **The outer NULL results will be generated with a delay that depends on the specified watermark delay and the time range condition. This is because the engine has to wait for that long to ensure there were no matches and there will be no more matches in future.
<strong><strong>Watermark will advance once new data is pushed to it</strong></strong></p>
</blockquote>
<p>Thus let’s generate some more fate data to the base table: **vehicle_geo. **This time we are sending a much lower volume of 10 records per second. Let the below command run for at least one batch and then kill it.</p>
<pre><code>stream_write_to_vehicle_geo_table(rowsPerSecond = 10, numPartitions = 10)
</code></pre>
<h3 id="5-b-what-to-observe">5. b What to observe:</h3>
<ol>
<li>
<p>Soon you should see the watermark moves ahead and the number of records in ‘Aggregation State’ goes down.</p>
</li>
<li>
<p>If you click on the running stream and click the raw data tab and look for “watermark”. You will see it has advanced</p>
</li>
<li>
<p>Once 0 records per second are being processed, that means your stream has caught up, and now your row count should match up with the traditional SQL left join</p>
<p>spark.read.table(f&rdquo;{schema_name}.{table_name_stream_stream_left_join}&rdquo;).count()</p>
</li>
</ol>
<h2 id="6-the-cold-start-edge-case-witheventtimeorder">6. The cold start edge case: withEventTimeOrder</h2>
<blockquote>
<p>“When using a Delta table as a stream source, the query first processes all of the data present in the table. The Delta table at this version is called the initial snapshot. By default, the Delta table’s data files are processed based on which file was last modified. However, the last modification time does not necessarily represent the record event time order.
In a stateful streaming query with a defined watermark, processing files by modification time can result in records being processed in the wrong order. This could lead to records dropping as late events by the watermark.
You can avoid the data drop issue by enabling the following option:
withEventTimeOrder: Whether the initial snapshot should be processed with event time order.</p>
</blockquote>
<p>In our scenario, I pushed this inside Step 3 when we created the temporary streaming views.</p>
<pre><code>spark.readStream.format(&quot;delta&quot;)
        .option(&quot;maxFilesPerTrigger&quot;, f&quot;{maxFilesPerTrigger}&quot;)
        .option(&quot;withEventTimeOrder&quot;, &quot;true&quot;)
        .table(f&quot;{schema_name}.{table_name}&quot;)
</code></pre>
<h2 id="7-cleanup">7. Cleanup</h2>
<p>Drop all tables in the database and delete all the checkpoints</p>
<pre><code>spark.sql(
    f&quot;&quot;&quot;
    drop schema if exists {schema_name} CASCADE
&quot;&quot;&quot;
)


dbutils.fs.rm(schema_storage_location, True)
</code></pre>
<p>If you have reached so far, you now have a working pipeline and a solid example which you can use going forward.</p>
<h2 id="download-the-code">Download the code</h2>
<p><a href="https://github.com/jiteshsoni/material_for_public_consumption/blob/main/notebooks/spark_stream_stream_join.py">https://github.com/jiteshsoni/material_for_public_consumption/blob/main/notebooks/spark_stream_stream_join.py</a></p>
<h3 id="references">References:</h3>
<ol>
<li>
<p><a href="https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins">https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#stream-stream-joins</a></p>
</li>
<li>
<p><a href="https://youtu.be/hyZU_bw1-ow?t=1181">https://youtu.be/hyZU_bw1-ow?t=1181</a></p>
</li>
<li>
<p><a href="https://www.youtube.com/watch?v=1cBDGsSbwRA&amp;t=1500s">https://www.youtube.com/watch?v=1cBDGsSbwRA&amp;t=1500s</a></p>
</li>
<li>
<p><a href="https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html">https://www.databricks.com/blog/2022/08/22/feature-deep-dive-watermarking-apache-spark-structured-streaming.html</a></p>
</li>
<li>
<p><a href="https://docs.databricks.com/structured-streaming/delta-lake.html#process-initial-snapshot-without-data-being-dropped">https://docs.databricks.com/structured-streaming/delta-lake.html#process-initial-snapshot-without-data-being-dropped</a></p>
</li>
</ol>
<h2 id="footnotes">Footnotes</h2>
<p>If you’re interested in learning more and keeping up to date with the latest about Spark, Delta, DBT, Python, SQL, Terraform, and other big data technologies, check out my <a href="https://canadiandataguy.medium.com/">other blogs and follow</a>.</p>

                        </div>
                        
                        

                    </div>
                    

                    

                    

                    <div class="col-md-3">

                        

                        













<nav class="sharebuttons">
    <span>:</span>
    <ul>
        
        
        
        <li><a href="https://t.me/+12065656859" target="_blank"><svg height="50" viewBox="0 0 50 50" width="50" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m25.4437935 0c13.5404221 0 24.5561688 10.9340909 24.5561688 24.3738636 0 13.4383117-11.0157467 24.3712663-24.5560065 24.3711039-4.0610389 0-8.0737013-1.0012987-11.624026-2.8982143l-12.97402593 4.1227273c-.06461039.0206169-.13084416.0305195-.19659091.0305195-.17159091 0-.33961039-.0681818-.46347403-.1946429-.17159091-.1746753-.23003247-.4311688-.15146104-.6631493l4.21801948-12.4423701c-2.20373376-3.724026-3.36704545-7.9790585-3.36704545-12.3259741 0-13.4397727 11.01688308-24.3738636 24.55844158-24.3738636zm-.0001624 5.07727273c-10.725974 0-19.45227269 8.65649347-19.45227269 19.29659087 0 4.0896104 1.28068182 8.0008117 3.70340909 11.3112013.125.1707793.15892857.3917208.09107143.5920455l-2.09805195 6.1897727 6.49918832-2.0659091c.064448-.0204545.1306818-.0305195.1967532-.0305195.125 0 .2491884.036039.3561689.1063312 3.1814935 2.0876624 6.8826298 3.1910714 10.7038961 3.1910714 10.7248376 0 19.4498376-8.6551948 19.4498376-19.2939935 0-10.6400974-8.7253246-19.29659087-19.45-19.29659087zm-6.6467532 7.85259737c.7225649 0 1.2425325.4384741 1.636039 1.3801948.1636363.3902598 1.7027597 4.1048702 1.788961 4.2772728.1050325.2060065.4300325.8439935.049513 1.6l-.0816559.1641233c-.1558441.3159091-.2902597.5886364-.5922078.9399351-.0969155.112013-.1956168.2308442-.2941558.349513-.2081169.250487-.4232143.5094156-.6217532.7058441-.0678572.0680195-.1993507.1998377-.2136364.2522728.000487 0 .0029221.0449675.0610389.1439935.4069806.6887987 3.375 4.7733766 7.2852273 6.4719156.1707792.0741883.7349026.3064935.7675325.3064935.0363636 0 .0970779-.0607143.1415584-.111526.3405844-.3849026 1.4428572-1.6733766 1.8069806-2.2146104.3220779-.4818182.7355519-.7271104 1.2277597-.7271104.3012987 0 .5845779.0926948.849026.1876624.6030564.2170398 3.8043369 1.7879312 4.3216328 2.0413055l.0686269.0335321c.4774351.2292208.8545455.4102273 1.0733766.7699676.3194805.5290584.1928572 1.9868506-.2873376 3.3211038-.609578 1.6939936-3.2894481 3.0826299-4.5089286 3.1907468l-.1788961.0170455c-.2819805.0277597-.6016234.0590909-1.000974.0590909l-.0944191-.0005452c-.9658464-.0112678-2.6415309-.2079278-6.0576913-1.5725068-3.6667208-1.4647727-7.2837663-4.6050325-10.1852273-8.8423701-.0501623-.0733766-.0847403-.124513-.1035714-.1496753-.7576299-.998539-2.5222403-3.624026-2.5222403-6.4047078 0-3.0896104 1.4788961-4.8464286 2.3530844-5.397565.8243507-.5196428 2.6887987-.7650974 3.0311689-.7795454.0351461-.0014814.0648513-.0027775.0900577-.0039117l.1047884-.0050457c.0548951-.0028933.0444193-.0028933.0863227-.0028933z" fill="#d5d5d5" class="will-change" fill-rule="nonzero"/><path d="m0 0h50v50h-50z"/></g></svg></a></li>
        
        <li><a href="https://canadiandataguy.medium.com/" target="_blank">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-medium" viewBox="0 0 16 16"> <path d="M9.025 8c0 2.485-2.02 4.5-4.513 4.5A4.506 4.506 0 0 1 0 8c0-2.486 2.02-4.5 4.512-4.5A4.506 4.506 0 0 1 9.025 8zm4.95 0c0 2.34-1.01 4.236-2.256 4.236-1.246 0-2.256-1.897-2.256-4.236 0-2.34 1.01-4.236 2.256-4.236 1.246 0 2.256 1.897 2.256 4.236zM16 8c0 2.096-.355 3.795-.794 3.795-.438 0-.793-1.7-.793-3.795 0-2.096.355-3.795.794-3.795.438 0 .793 1.699.793 3.795z"/> </svg>         
          </a></li>
        <li><a href="mailto:info@canadiandataguy.com" target="_blank"><svg height="50" viewBox="0 0 50 50" width="50" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m46.149789 6h-42.29957803c-2.12025316 0-3.85021097 1.72995781-3.85021097 3.85021097v29.78902953c0 2.1202532 1.72995781 3.850211 3.85021097 3.850211h42.29957803c2.1202532 0 3.850211-1.7299578 3.850211-3.850211v-29.78902953c0-2.12025316-1.7299578-3.85021097-3.850211-3.85021097zm-.5590717 2.84810127-20.3270042 17.38396623-20.32700424-17.38396623zm1.5611814 30.79113923c0 .5485232-.4535865 1.0021097-1.0021097 1.0021097h-42.29957803c-.54852321 0-1.0021097-.4535865-1.0021097-1.0021097v-28.8291139l21.48734173 18.3755274c.0105486.0105485.0316456.0210971.0421941.0316456.0105486.0105485.0316456.021097.0421941.0316455.0316456.0210971.0527426.0421941.0843882.0527427.0105485.0105485.0210971.0105485.0316456.021097.0421941.0210971.0843882.0421941.1265823.0632911.0105485 0 .021097.0105486.0316455.0105486.0316456.0105485.0632912.0316455.1054853.0421941.0105485 0 .0316455.0105485.0421941.0105485.0316455.0105485.0632911.021097.0949367.021097.0105485 0 .0316455.0105486.0421941.0105486.0316455.0105485.0738396.0105485.1054852.021097h.0316456c.042194 0 .0949367.0105485.1371308.0105485.042194 0 .0949367 0 .1371308-.0105485h.0316455c.0316456 0 .0738397-.0105485.1054853-.021097.0105485 0 .0316455-.0105486.0421941-.0105486.0316455-.0105485.0632911-.021097.0949367-.021097.0105485 0 .0316455-.0105485.0421941-.0105485.0316455-.0105486.0632911-.0210971.1054852-.0421941.0105485 0 .021097-.0105486.0316456-.0105486.042194-.021097.0843881-.042194.1265822-.0632911.0105486-.0105485.0210971-.0105485.0316456-.021097.0316456-.0210971.0527426-.0316456.0843882-.0527427.0105485-.0105485.0316456-.021097.0421941-.0316455s.0316456-.0210971.0421941-.0316456l20.9810126-17.9219409z" fill="#d5d5d5" class="will-change" fill-rule="nonzero"/><path d="m0 0h50v50h-50z"/></g></svg></a></li>
    </ul>
</nav>
<style>
.sharebuttons, .sharebuttons ul {display: flex; margin: 0.25rem 0; align-items: center;}
.sharebuttons span {color: #bbb;}
.sharebuttons ul li {list-style: none; margin: 0;}
.sharebuttons ul li a {margin: 0 0.5rem; display: block;}
.sharebuttons ul li:first-child a {margin: 0 0.2rem 0 0.5rem;}
.sharebuttons ul li a svg {width: auto; height:4.1rem; display: block;}
.will-change {transition: 250ms;} 
nav.sharebuttons li > a:hover .will-change { fill: #444444 }
</style>



<div class="panel panel-default sidebar-menu">

    <div class="panel-heading">
        <h3 class="panel-title">Categories</h3>
    </div>

    <div class="panel-body">
        <ul class="nav nav-pills nav-stacked">
            
            
            <li>
                <a href="/categories/best-practices">BEST-PRACTICES (2)</a>
            </li>
            
            <li>
                <a href="/categories/coaching">COACHING (3)</a>
            </li>
            
            <li>
                <a href="/categories/customer-stories">CUSTOMER-STORIES (2)</a>
            </li>
            
            <li>
                <a href="/categories/databricks">DATABRICKS (14)</a>
            </li>
            
            <li>
                <a href="/categories/interviewing">INTERVIEWING (1)</a>
            </li>
            
            <li>
                <a href="/categories/spark">SPARK (4)</a>
            </li>
            
            <li>
                <a href="/categories/spark-streaming">SPARK-STREAMING (7)</a>
            </li>
            
            <li>
                <a href="/categories/streaming">STREAMING (8)</a>
            </li>
            
        </ul>
    </div>

</div>








<div class="panel sidebar-menu">

    <div class="panel-heading">
        <h3 class="panel-title">Tags</h3>
    </div>

    <div class="panel-body">
        <ul class="tag-cloud">
            
            
            <li >
                <a href="/tags/books"><i class="fas fa-tags"></i> books</a>
            </li>
            
            <li >
                <a href="/tags/checkpoint"><i class="fas fa-tags"></i> checkpoint</a>
            </li>
            
            <li >
                <a href="/tags/coaching"><i class="fas fa-tags"></i> coaching</a>
            </li>
            
            <li >
                <a href="/tags/concurrency"><i class="fas fa-tags"></i> concurrency</a>
            </li>
            
            <li >
                <a href="/tags/cost-savings"><i class="fas fa-tags"></i> cost-savings</a>
            </li>
            
            <li >
                <a href="/tags/dbsql"><i class="fas fa-tags"></i> dbsql</a>
            </li>
            
            <li >
                <a href="/tags/delta"><i class="fas fa-tags"></i> delta</a>
            </li>
            
            <li >
                <a href="/tags/delta-live-tables"><i class="fas fa-tags"></i> delta-live-tables</a>
            </li>
            
            <li >
                <a href="/tags/foreachbatch"><i class="fas fa-tags"></i> foreachbatch</a>
            </li>
            
            <li >
                <a href="/tags/graviton"><i class="fas fa-tags"></i> graviton</a>
            </li>
            
            <li >
                <a href="/tags/interviewing"><i class="fas fa-tags"></i> interviewing</a>
            </li>
            
            <li >
                <a href="/tags/job_id"><i class="fas fa-tags"></i> job_id</a>
            </li>
            
            <li >
                <a href="/tags/kafka"><i class="fas fa-tags"></i> kafka</a>
            </li>
            
            <li >
                <a href="/tags/merge"><i class="fas fa-tags"></i> merge</a>
            </li>
            
            <li >
                <a href="/tags/optimize"><i class="fas fa-tags"></i> optimize</a>
            </li>
            
            <li >
                <a href="/tags/parallelization"><i class="fas fa-tags"></i> parallelization</a>
            </li>
            
            <li >
                <a href="/tags/python"><i class="fas fa-tags"></i> python</a>
            </li>
            
            <li >
                <a href="/tags/run_id"><i class="fas fa-tags"></i> run_id</a>
            </li>
            
            <li >
                <a href="/tags/spark"><i class="fas fa-tags"></i> spark</a>
            </li>
            
            <li >
                <a href="/tags/spark-streaming"><i class="fas fa-tags"></i> spark-streaming</a>
            </li>
            
            <li >
                <a href="/tags/streaming"><i class="fas fa-tags"></i> streaming</a>
            </li>
            
            <li >
                <a href="/tags/workspace"><i class="fas fa-tags"></i> workspace</a>
            </li>
            
            <li >
                <a href="/tags/youtube"><i class="fas fa-tags"></i> youtube</a>
            </li>
            
            <li >
                <a href="/tags/z-order"><i class="fas fa-tags"></i> z-order</a>
            </li>
            
        </ul>
    </div>

</div>







                        

                    </div>
                    

                    

                </div>
                

            </div>
            
        </div>
        

        <footer id="footer">
    <div class="container">

        
        <div class="col-md-4 col-sm-6">
            <h4>About us</h4>

            <p>We offer expertise and consulting in data engineering, analytics and cloud computing.</p>

            <hr class="hidden-md hidden-lg hidden-sm">

        </div>
        
        

        <div class="col-md-4 col-sm-6">

            

        </div>
        

        
        <div class="col-md-4 col-sm-6">

          <h4>Contact</h4>

            <p class="text-uppercase"><strong>Canadian Data Guy Corp.</strong>
        <br>Toronto, Canada
        </p>
      

	    <a href="/contact" class="btn btn-small btn-template-main">Go to contact page</a>

            <hr class="hidden-md hidden-lg hidden-sm">

        </div>
        









<nav class="sharebuttons">
    <span>:</span>
    <ul>
        
        
        
        <li><a href="https://t.me/+12065656859" target="_blank"><svg height="50" viewBox="0 0 50 50" width="50" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m25.4437935 0c13.5404221 0 24.5561688 10.9340909 24.5561688 24.3738636 0 13.4383117-11.0157467 24.3712663-24.5560065 24.3711039-4.0610389 0-8.0737013-1.0012987-11.624026-2.8982143l-12.97402593 4.1227273c-.06461039.0206169-.13084416.0305195-.19659091.0305195-.17159091 0-.33961039-.0681818-.46347403-.1946429-.17159091-.1746753-.23003247-.4311688-.15146104-.6631493l4.21801948-12.4423701c-2.20373376-3.724026-3.36704545-7.9790585-3.36704545-12.3259741 0-13.4397727 11.01688308-24.3738636 24.55844158-24.3738636zm-.0001624 5.07727273c-10.725974 0-19.45227269 8.65649347-19.45227269 19.29659087 0 4.0896104 1.28068182 8.0008117 3.70340909 11.3112013.125.1707793.15892857.3917208.09107143.5920455l-2.09805195 6.1897727 6.49918832-2.0659091c.064448-.0204545.1306818-.0305195.1967532-.0305195.125 0 .2491884.036039.3561689.1063312 3.1814935 2.0876624 6.8826298 3.1910714 10.7038961 3.1910714 10.7248376 0 19.4498376-8.6551948 19.4498376-19.2939935 0-10.6400974-8.7253246-19.29659087-19.45-19.29659087zm-6.6467532 7.85259737c.7225649 0 1.2425325.4384741 1.636039 1.3801948.1636363.3902598 1.7027597 4.1048702 1.788961 4.2772728.1050325.2060065.4300325.8439935.049513 1.6l-.0816559.1641233c-.1558441.3159091-.2902597.5886364-.5922078.9399351-.0969155.112013-.1956168.2308442-.2941558.349513-.2081169.250487-.4232143.5094156-.6217532.7058441-.0678572.0680195-.1993507.1998377-.2136364.2522728.000487 0 .0029221.0449675.0610389.1439935.4069806.6887987 3.375 4.7733766 7.2852273 6.4719156.1707792.0741883.7349026.3064935.7675325.3064935.0363636 0 .0970779-.0607143.1415584-.111526.3405844-.3849026 1.4428572-1.6733766 1.8069806-2.2146104.3220779-.4818182.7355519-.7271104 1.2277597-.7271104.3012987 0 .5845779.0926948.849026.1876624.6030564.2170398 3.8043369 1.7879312 4.3216328 2.0413055l.0686269.0335321c.4774351.2292208.8545455.4102273 1.0733766.7699676.3194805.5290584.1928572 1.9868506-.2873376 3.3211038-.609578 1.6939936-3.2894481 3.0826299-4.5089286 3.1907468l-.1788961.0170455c-.2819805.0277597-.6016234.0590909-1.000974.0590909l-.0944191-.0005452c-.9658464-.0112678-2.6415309-.2079278-6.0576913-1.5725068-3.6667208-1.4647727-7.2837663-4.6050325-10.1852273-8.8423701-.0501623-.0733766-.0847403-.124513-.1035714-.1496753-.7576299-.998539-2.5222403-3.624026-2.5222403-6.4047078 0-3.0896104 1.4788961-4.8464286 2.3530844-5.397565.8243507-.5196428 2.6887987-.7650974 3.0311689-.7795454.0351461-.0014814.0648513-.0027775.0900577-.0039117l.1047884-.0050457c.0548951-.0028933.0444193-.0028933.0863227-.0028933z" fill="#d5d5d5" class="will-change" fill-rule="nonzero"/><path d="m0 0h50v50h-50z"/></g></svg></a></li>
        
        <li><a href="https://canadiandataguy.medium.com/" target="_blank">
          <svg xmlns="http://www.w3.org/2000/svg" width="16" height="16" fill="currentColor" class="bi bi-medium" viewBox="0 0 16 16"> <path d="M9.025 8c0 2.485-2.02 4.5-4.513 4.5A4.506 4.506 0 0 1 0 8c0-2.486 2.02-4.5 4.512-4.5A4.506 4.506 0 0 1 9.025 8zm4.95 0c0 2.34-1.01 4.236-2.256 4.236-1.246 0-2.256-1.897-2.256-4.236 0-2.34 1.01-4.236 2.256-4.236 1.246 0 2.256 1.897 2.256 4.236zM16 8c0 2.096-.355 3.795-.794 3.795-.438 0-.793-1.7-.793-3.795 0-2.096.355-3.795.794-3.795.438 0 .793 1.699.793 3.795z"/> </svg>         
          </a></li>
        <li><a href="mailto:info@canadiandataguy.com" target="_blank"><svg height="50" viewBox="0 0 50 50" width="50" xmlns="http://www.w3.org/2000/svg"><g fill="none" fill-rule="evenodd"><path d="m46.149789 6h-42.29957803c-2.12025316 0-3.85021097 1.72995781-3.85021097 3.85021097v29.78902953c0 2.1202532 1.72995781 3.850211 3.85021097 3.850211h42.29957803c2.1202532 0 3.850211-1.7299578 3.850211-3.850211v-29.78902953c0-2.12025316-1.7299578-3.85021097-3.850211-3.85021097zm-.5590717 2.84810127-20.3270042 17.38396623-20.32700424-17.38396623zm1.5611814 30.79113923c0 .5485232-.4535865 1.0021097-1.0021097 1.0021097h-42.29957803c-.54852321 0-1.0021097-.4535865-1.0021097-1.0021097v-28.8291139l21.48734173 18.3755274c.0105486.0105485.0316456.0210971.0421941.0316456.0105486.0105485.0316456.021097.0421941.0316455.0316456.0210971.0527426.0421941.0843882.0527427.0105485.0105485.0210971.0105485.0316456.021097.0421941.0210971.0843882.0421941.1265823.0632911.0105485 0 .021097.0105486.0316455.0105486.0316456.0105485.0632912.0316455.1054853.0421941.0105485 0 .0316455.0105485.0421941.0105485.0316455.0105485.0632911.021097.0949367.021097.0105485 0 .0316455.0105486.0421941.0105486.0316455.0105485.0738396.0105485.1054852.021097h.0316456c.042194 0 .0949367.0105485.1371308.0105485.042194 0 .0949367 0 .1371308-.0105485h.0316455c.0316456 0 .0738397-.0105485.1054853-.021097.0105485 0 .0316455-.0105486.0421941-.0105486.0316455-.0105485.0632911-.021097.0949367-.021097.0105485 0 .0316455-.0105485.0421941-.0105485.0316455-.0105486.0632911-.0210971.1054852-.0421941.0105485 0 .021097-.0105486.0316456-.0105486.042194-.021097.0843881-.042194.1265822-.0632911.0105486-.0105485.0210971-.0105485.0316456-.021097.0316456-.0210971.0527426-.0316456.0843882-.0527427.0105485-.0105485.0316456-.021097.0421941-.0316455s.0316456-.0210971.0421941-.0316456l20.9810126-17.9219409z" fill="#d5d5d5" class="will-change" fill-rule="nonzero"/><path d="m0 0h50v50h-50z"/></g></svg></a></li>
    </ul>
</nav>
<style>
.sharebuttons, .sharebuttons ul {display: flex; margin: 0.25rem 0; align-items: center;}
.sharebuttons span {color: #bbb;}
.sharebuttons ul li {list-style: none; margin: 0;}
.sharebuttons ul li a {margin: 0 0.5rem; display: block;}
.sharebuttons ul li:first-child a {margin: 0 0.2rem 0 0.5rem;}
.sharebuttons ul li a svg {width: auto; height:4.1rem; display: block;}
.will-change {transition: 250ms;} 
nav.sharebuttons li > a:hover .will-change { fill: #444444 }
</style>
        
        

    </div>
    
</footer>







<div id="copyright">
    <div class="container">
        <div class="col-md-12">
            
            <p class="pull-left">Copyright (c) 2023, Canadian Data Guy Corp; all rights reserved.</p>
            
            <p class="pull-right">
              Template by <a href="https://bootstrapious.com/p/universal-business-e-commerce-template">Bootstrapious</a>.
              

              Ported to Hugo by <a href="https://github.com/devcows/hugo-universal-theme">DevCows</a>.
            </p>
        </div>
    </div>
</div>





    </div>
    

    
<script src="//code.jquery.com/jquery-3.1.1.min.js" integrity="sha256-hVVnYaiADRTO2PzUGmuLJr8BLUSjGIZsDYGmIJLv2b8=" crossorigin="anonymous"></script>
<script src="//maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script>

<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-cookie/1.4.1/jquery.cookie.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/waypoints/4.0.1/jquery.waypoints.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/Counter-Up/1.0/jquery.counterup.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/jquery-parallax/1.1.3/jquery-parallax.js"></script>


<script src="/js/front.js"></script>


<script src="/js/owl.carousel.min.js"></script>



  </body>
</html>
