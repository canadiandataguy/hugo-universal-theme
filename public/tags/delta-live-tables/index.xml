<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>delta live tables on Canadian Data Guy</title>
    <link>https://canadiandataguy.com/tags/delta-live-tables/</link>
    <description>Recent content in delta live tables on Canadian Data Guy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 03 Mar 2023 17:35:21 -0500</lastBuildDate>
    <atom:link href="https://canadiandataguy.com/tags/delta-live-tables/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Delta Live Tables Advanced Q &amp; A</title>
      <link>https://canadiandataguy.com/blog/deltalivetablesadvancedqa/</link>
      <pubDate>Fri, 03 Mar 2023 17:35:21 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/deltalivetablesadvancedqa/</guid>
      <description>Delta Live Tables Advanced Q &amp;amp; A This is primarily written for those trying to handle edge cases.
Q1.) How can a single/unified table be built with historical backfill and ongoing streaming Kafka data? The streaming table built using DLT allows writes to the table outside of the DLT. Thus, you can build and run your DLT pipeline with Kafka as a source, generating the physical table with a name. Then, you can do a streaming write to this table outside DLT.</description>
    </item>
    <item>
      <title>How to upgrade your Spark Stream application with a new checkpoint!</title>
      <link>https://canadiandataguy.com/blog/howtoupgradeyoursparkstreamapplicationwithanewcheckpoint/</link>
      <pubDate>Wed, 25 Jan 2023 17:35:21 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/howtoupgradeyoursparkstreamapplicationwithanewcheckpoint/</guid>
      <description>How to upgrade your Spark Stream application with a new checkpoint With working code Sometimes in life, we need to make breaking changes which require us to create a new checkpoint. Some example scenarios:
You are doing a code/application change where you are changing logic
Major Spark Version upgrade from Spark 2.x to Spark 3.x
The previous deployment was wrong, and you want to reprocess from a certain point
There could be plenty of scenarios where you want to control precisely which data(Kafka offsets) need to be processed.</description>
    </item>
    <item>
      <title>How to parameterize Delta Live Tables and import reusable functions</title>
      <link>https://canadiandataguy.com/blog/howtoparameterizedeltalivetablesandimportreusablefunctions/</link>
      <pubDate>Tue, 13 Dec 2022 17:35:21 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/howtoparameterizedeltalivetablesandimportreusablefunctions/</guid>
      <description>How to parameterize Delta Live Tables and import reusable functions with working code This blog will discuss passing custom parameters to a Delta Live Tables (DLT) pipeline. Furthermore, we will discuss importing functions defined in other files or locations. You can import files from the current directory or a specified location using sys.path.append().
Update: As of *December 2022, you can directly import files if the reusable_functions.py file exists in the same repository by just using the import command, which is the preferred approach.</description>
    </item>
    <item>
      <title>How Audantic Uses Databricks Delta Live Tables to Increase Productivity for Real Estate Market Segments</title>
      <link>https://canadiandataguy.com/blog/audantic/</link>
      <pubDate>Thu, 05 May 2022 17:35:21 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/audantic/</guid>
      <description>To support our data-driven initiatives, we had ‘stitched’ together various services for ETL, orchestration, ML leveraging AWS, Airflow, where we saw some success but quickly turned into an overly complex system that took nearly five times as long to develop compared to the new solution. Our team captured high-level metrics comparing our previous implementation and current lakehouse solution. As you can see from the table below, we spent months developing our previous solution and had to write approximately 3 times as much code.</description>
    </item>
  </channel>
</rss>
