<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>kafka on Canadian Data Guy</title>
    <link>https://canadiandataguy.com/tags/kafka/</link>
    <description>Recent content in kafka on Canadian Data Guy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 25 Jan 2023 17:35:21 -0500</lastBuildDate>
    <atom:link href="https://canadiandataguy.com/tags/kafka/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>How to upgrade your Spark Stream application with a new checkpoint!</title>
      <link>https://canadiandataguy.com/blog/howtoupgradeyoursparkstreamapplicationwithanewcheckpoint/</link>
      <pubDate>Wed, 25 Jan 2023 17:35:21 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/howtoupgradeyoursparkstreamapplicationwithanewcheckpoint/</guid>
      <description>How to upgrade your Spark Stream application with a new checkpoint With working code Sometimes in life, we need to make breaking changes which require us to create a new checkpoint. Some example scenarios:
You are doing a code/application change where you are changing logic
Major Spark Version upgrade from Spark 2.x to Spark 3.x
The previous deployment was wrong, and you want to reprocess from a certain point
There could be plenty of scenarios where you want to control precisely which data(Kafka offsets) need to be processed.</description>
    </item>
    <item>
      <title>Using Spark Streaming to merge/upsert data into a Delta Lake with working code</title>
      <link>https://canadiandataguy.com/blog/usingsparkstreamingtomergeupsertdataintoadeltalakewithworkingcode/</link>
      <pubDate>Wed, 12 Oct 2022 04:06:14 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/usingsparkstreamingtomergeupsertdataintoadeltalakewithworkingcode/</guid>
      <description>Using Spark Streaming to merge/upsert data into a Delta Lake with working code This blog will discuss how to read from a Spark Streaming and merge/upsert data into a Delta Lake. We will also optimize/cluster data of the delta table. In the end, we will show how to start a streaming pipeline with the previous target table as the source.
Overall, the process works in the following manner, we read data from a streaming source and use this special function ***foreachBatch.</description>
    </item>
  </channel>
</rss>
