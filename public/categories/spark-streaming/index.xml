<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>spark streaming on Canadian Data Guy</title>
    <link>https://canadiandataguy.com/categories/spark-streaming/</link>
    <description>Recent content in spark streaming on Canadian Data Guy</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 19 Apr 2023 01:04:45 +0000</lastBuildDate>
    <atom:link href="https://canadiandataguy.com/categories/spark-streaming/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Spark Streaming Best Practices-A bare minimum checklist for Beginners and Advanced Users</title>
      <link>https://canadiandataguy.com/blog/2023-04-18-spark-streaming-best-practices-a-bare-minimum-checklist-for-beginners-and-advanced-users/</link>
      <pubDate>Wed, 19 Apr 2023 01:04:45 +0000</pubDate>
      <guid>https://canadiandataguy.com/blog/2023-04-18-spark-streaming-best-practices-a-bare-minimum-checklist-for-beginners-and-advanced-users/</guid>
      <description>Spark Streaming Best Practices-A bare minimum checklist for Beginners and Advanced Users Most good things in life come with a nuance. While learning Streaming a few years ago, I spent hours searching for best practices. However, I would find answers to be complicated to make sense for a beginnerâ€™s mind. Thus, I devised a set of best practices that should hold true in almost all scenarios.
The below checklist is not ordered, you should aim to check off as many items as you can.</description>
    </item>
    <item>
      <title>Delta Live Tables Advanced Q &amp; A</title>
      <link>https://canadiandataguy.com/blog/deltalivetablesadvancedqa/</link>
      <pubDate>Fri, 03 Mar 2023 17:35:21 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/deltalivetablesadvancedqa/</guid>
      <description>Delta Live Tables Advanced Q &amp;amp; A This is primarily written for those trying to handle edge cases.
Q1.) How can a single/unified table be built with historical backfill and ongoing streaming Kafka data? The streaming table built using DLT allows writes to the table outside of the DLT. Thus, you can build and run your DLT pipeline with Kafka as a source, generating the physical table with a name. Then, you can do a streaming write to this table outside DLT.</description>
    </item>
    <item>
      <title>How I wrote my first Spark Streaming Application with Joins?</title>
      <link>https://canadiandataguy.com/blog/howiwrotemyfirstsparkstreamingapplicationwithjoins/</link>
      <pubDate>Wed, 25 Jan 2023 17:35:21 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/howiwrotemyfirstsparkstreamingapplicationwithjoins/</guid>
      <description>How I wrote my first Spark Streaming Application with Joins with working code When I started learning about Spark Streaming, I could not find enough code/material which could kick-start my journey and build my confidence. I wrote this blog to fill this gap which could help beginners understand how simple streaming is and build their first application.
In this blog, I will explain most things by first principles to increase your understanding and confidence and you walk away with code for your first Streaming application.</description>
    </item>
    <item>
      <title>How to upgrade your Spark Stream application with a new checkpoint!</title>
      <link>https://canadiandataguy.com/blog/howtoupgradeyoursparkstreamapplicationwithanewcheckpoint/</link>
      <pubDate>Wed, 25 Jan 2023 17:35:21 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/howtoupgradeyoursparkstreamapplicationwithanewcheckpoint/</guid>
      <description>How to upgrade your Spark Stream application with a new checkpoint With working code Sometimes in life, we need to make breaking changes which require us to create a new checkpoint. Some example scenarios:
You are doing a code/application change where you are changing logic
Major Spark Version upgrade from Spark 2.x to Spark 3.x
The previous deployment was wrong, and you want to reprocess from a certain point
There could be plenty of scenarios where you want to control precisely which data(Kafka offsets) need to be processed.</description>
    </item>
    <item>
      <title>How to parameterize Delta Live Tables and import reusable functions</title>
      <link>https://canadiandataguy.com/blog/howtoparameterizedeltalivetablesandimportreusablefunctions/</link>
      <pubDate>Tue, 13 Dec 2022 17:35:21 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/howtoparameterizedeltalivetablesandimportreusablefunctions/</guid>
      <description>How to parameterize Delta Live Tables and import reusable functions with working code This blog will discuss passing custom parameters to a Delta Live Tables (DLT) pipeline. Furthermore, we will discuss importing functions defined in other files or locations. You can import files from the current directory or a specified location using sys.path.append().
Update: As of *December 2022, you can directly import files if the reusable_functions.py file exists in the same repository by just using the import command, which is the preferred approach.</description>
    </item>
    <item>
      <title>Merge Multiple Spark Streams Into A Delta Table</title>
      <link>https://canadiandataguy.com/blog/mergemultiplesparkstreamsintoadeltatable/</link>
      <pubDate>Thu, 13 Oct 2022 04:09:03 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/mergemultiplesparkstreamsintoadeltatable/</guid>
      <description>Merge Multiple Spark Streams Into A Delta Table with working code This blog will discuss how to read from multiple Spark Streams and merge/upsert data into a single Delta Table. We will also optimize/cluster data of the delta table.
Overall, the process works in the following manner:
Read data from a streaming source
Use this special function ***foreachBatch. ***Using this we will call any user-defined function responsible for all the processing.</description>
    </item>
    <item>
      <title>Using Spark Streaming to merge/upsert data into a Delta Lake with working code</title>
      <link>https://canadiandataguy.com/blog/usingsparkstreamingtomergeupsertdataintoadeltalakewithworkingcode/</link>
      <pubDate>Wed, 12 Oct 2022 04:06:14 -0500</pubDate>
      <guid>https://canadiandataguy.com/blog/usingsparkstreamingtomergeupsertdataintoadeltalakewithworkingcode/</guid>
      <description>Using Spark Streaming to merge/upsert data into a Delta Lake with working code This blog will discuss how to read from a Spark Streaming and merge/upsert data into a Delta Lake. We will also optimize/cluster data of the delta table. In the end, we will show how to start a streaming pipeline with the previous target table as the source.
Overall, the process works in the following manner, we read data from a streaming source and use this special function ***foreachBatch.</description>
    </item>
  </channel>
</rss>
